{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Scene Recognition with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: bags of SIFT descriptors\n",
    "### Question 4a: clustering SIFT descriptors with K-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(ds_path):\n",
    "    \"\"\" Load from the training/testing dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_path: path to the training/testing dataset.\n",
    "             e.g., sift/train or sift/test \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image_paths: a (n_sample, 1) array that contains the paths to the descriptors. \n",
    "    labels: class labels corresponding to each image\n",
    "    \"\"\"\n",
    "    # Grab a list of paths that matches the pathname\n",
    "    files = glob.glob(os.path.join(ds_path, \"*\", \"*.txt\"))\n",
    "    n_files = len(files)\n",
    "    image_paths = np.asarray(files)\n",
    " \n",
    "    # Get class labels\n",
    "    classes = glob.glob(os.path.join(ds_path, \"*\"))\n",
    "    labels = np.zeros(n_files)\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        folder, fn = os.path.split(path)\n",
    "        labels[i] = np.argwhere(np.core.defchararray.equal(classes, folder))[0,0]\n",
    "\n",
    "    # Randomize the order\n",
    "    idx = np.random.choice(n_files, size=n_files, replace=False)\n",
    "    image_paths = image_paths[idx]\n",
    "    labels = labels[idx]\n",
    "    return image_paths, labels\n",
    "\n",
    "def build_vocabulary(image_paths, vocab_size):\n",
    "    \"\"\" Sample SIFT descriptors, cluster them using k-means, and return the fitted k-means model.\n",
    "    NOTE: We don't necessarily need to use the entire training dataset. You can use the function\n",
    "    sample_images() to sample a subset of images, and pass them into this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_paths: an (n_image, 1) array of image paths.\n",
    "    vocab_size: the number of clusters desired.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    kmeans: the fitted k-means clustering model.\n",
    "    \"\"\"\n",
    "    n_image = len(image_paths)\n",
    "\n",
    "    # Since want to sample tens of thousands of SIFT descriptors from different images, we\n",
    "    # calculate the number of SIFT descriptors we need to sample from each image.\n",
    "    n_each = int(np.ceil(10000 / n_image))\n",
    "\n",
    "    # Initialize an array of features, which will store the sampled descriptors\n",
    "    # keypoints = np.zeros((n_image * n_each, 2))\n",
    "    descriptors = np.zeros((n_image * n_each, 128))\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        # Load features from each image\n",
    "        features = np.loadtxt(path, delimiter=',',dtype=float)\n",
    "        sift_keypoints = features[:, :2]\n",
    "        sift_descriptors = features[:, 2:]\n",
    "\n",
    "        # TODO: Randomly sample n_each descriptors from sift_descriptor and store them into descriptors\n",
    "\n",
    "    # TODO: pefrom k-means clustering to cluster sampled sift descriptors into vocab_size regions.\n",
    "    # You can use KMeans from sci-kit learn.\n",
    "    # Reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "    \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Getting paths and labels for all train and test data')\n",
    "train_image_paths, train_labels = load(\"sift/train\")\n",
    "test_image_paths, test_labels = load(\"sift/test\")\n",
    "\n",
    "print('Extracting SIFT features')\n",
    "kmeans = build_vocabulary(train_image_paths, vocab_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b: representing images as bags of SIFT feature histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bags_of_sifts(image_paths, kmeans):\n",
    "    \"\"\" Represent each image as bags of SIFT features histogram.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_paths: an (n_image, 1) array of image paths.\n",
    "    kmeans: k-means clustering model with vocab_size centroids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image_feats: an (n_image, vocab_size) matrix, where each row is a histogram.\n",
    "    \"\"\"\n",
    "    n_image = len(image_paths)\n",
    "    vocab_size = kmeans.cluster_centers_.shape[0]\n",
    "\n",
    "    image_feats = np.zeros((n_image, vocab_size))\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        # Load features from each image\n",
    "        features = np.loadtxt(path, delimiter=',',dtype=float)\n",
    "\n",
    "        # TODO: Assign each feature to the closest cluster center\n",
    "        # Again, each feature consists of the (x, y) location and the 128-dimensional sift descriptor\n",
    "        # You can access the sift descriptors part by features[:, 2:]\n",
    "\n",
    "        # TODO: Build a histogram normalized by the number of descriptors\n",
    "\n",
    "    return image_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_feats = get_bags_of_sifts(train_image_paths, kmeans)\n",
    "test_image_feats = get_bags_of_sifts(test_image_paths, kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4c: average histogram for each scene category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: scene recongition with KNN\n",
    "This function will predict the category for every test image by finding the training image with most similar features. Instead of 1 nearest neighbor, you can vote based on k nearest neighbors which will increase performance (although you need to pick a reasonable value for k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_image_feats:  is an N x d matrix, where d is the dimensionality of the feature representation.\n",
    "    train_labels: is an N x l cell array, where each entry is a string\n",
    "                  indicating the ground truth one-hot vector for each training image.\n",
    "    test_image_feats: is an M x d matrix, where d is the dimensionality of the\n",
    "                      feature representation. You can assume M = N unless you've modified the starter code.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is an M x l cell array, where each row is a one-hot vector\n",
    "    indicating the predicted category for each test image.\n",
    "    \"\"\"\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using nearest neighbor classifier to predict test set categories')\n",
    "pred_labels_knn = nearest_neighbor_classify(train_image_feats, train_labels, test_image_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: scene recognition with 1-vs-all linear SVMs\n",
    "This function will train a linear SVM for every category (i.e. one vs all) and then use the learned linear classifiers to predict the category of very test image. Every test feature will be evaluated with all 15 SVMs and the most confident SVM will \"win\". Confidence, or distance from the margin, is W*X + B where '*' is the inner product or dot product and W and B are the learned hyperplane parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_image_feats:  is an N x d matrix, where d is the dimensionality of the feature representation.\n",
    "    train_labels: is an N x l cell array, where each entry is a string\n",
    "                  indicating the ground truth one-hot vector for each training image.\n",
    "    test_image_feats: is an M x d matrix, where d is the dimensionality of the\n",
    "                      feature representation. You can assume M = N unless you've modified the starter code.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is an M x l cell array, where each row is a one-hot vector\n",
    "    indicating the predicted category for each test image.\n",
    "    \"\"\"\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using support vector machine to predict test set categories')\n",
    "pred_labels_svm = svm_classify(train_image_feats, train_labels, test_image_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
